# MLP Model Configuration
name: mlp

# Architecture
input_size: 3072  # 32x32x3 for CIFAR-10
hidden_dims: [512, 512]
num_classes: 10
dropout: 0.2
activation: relu

# Optimization
learning_rate: 0.001
optimizer: adam
weight_decay: 0.0001

# Optimizer specific parameters
adam:
  betas: [0.9, 0.999]
  eps: 1.0e-8

# Learning rate scheduler (optional)
scheduler:
  use: false
  type: step
  step_size: 30
  gamma: 0.1
